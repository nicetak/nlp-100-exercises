# Chapter 4: POS Tagging

```{bash, include=FALSE}
if [ ! -f ../data/alice.txt ]
then
wget -nc -O ../data/alice.zip https://nlp100.github.io/data/alice.zip
unzip ../data/alice.zip -d ../data/
rm ../data/readme.alice.md ../data/alice.txt.json ../data/build_alice.sh ../data/11.txt ../data/alice.zip
fi
```

```{r setup, include=FALSE}
rm(list = ls())
library(tidyverse)
library(here)
```

## 30. Reading the result
>Implement a program that reads the result of part-of-speech tagging. Here, represent a sentence as a list of mapping objects, each of which associates a surface form, lemma (base form), part-of-speech tag with the keys text, lemma, pos. Use this representation in the rest of the problems.

```{r, cache=TRUE}
lines <- readLines(here("data/alice.txt.conll"))

alice <- tibble(id_sentence = numeric(),
                text = character(),
                lemma = character(),
                pos = character())
id_sentence <- 1
for(line in lines){
  if(line == ""){
    id_sentence <- id_sentence + 1
  } else{
    x <- str_split(line, "\t")[[1]]
    alice <- alice %>%
      add_row(id_sentence = id_sentence, text = x[2], lemma = x[3], pos = x[4])
  }
}

alice <- alice %>%
  rowid_to_column("id")

alice %>%
  head()
```

## 31. Verbs
>Extract surface forms of all verbs appearing in the text.

```{r}
alice %>%
  filter(str_detect(pos, "VB")) %>%
  pull(text) %>%
  head()
```

## 32. Base forms of verbs
>Extract lemmas of all verbs appearing in the text.

```{r}
alice %>%
  filter(str_detect(pos, "VB")) %>%
  pull(lemma) %>%
  head()
```

## 33. A of B
>Extract noun phrases in the form of “A of B”, where A and B are nouns.

```{r}
is_a_of_b <- function(i){
  alice$pos[i] == "NN" & alice$text[i+1] == "of" & alice$pos[i+2] %in% "NN"
}

a_of_b <- c()
for(i in 1:(nrow(alice)-2)){
  if(is_a_of_b(i)){
    a_of_b <- c(a_of_b, sprintf("%s of %s", alice$text[i], alice$text[i+2]))
  }
}

a_of_b
```

## 34. A B
>Extract the longest noun phrase consisting of consecutive nouns.

```{r}
v <- alice %>%
  filter(pos == "NN") %>%
  pull(id)

temp <- cumsum(c(1, diff(v) - 1))
temp2 <- rle(temp)
ids <- v[which(temp == with(temp2, values[which.max(lengths)]))]

alice[alice$id %in% ids, ]$text
```

## 35. Frequency of words
>Obtain the list of words and frequencies of their occurrences sorted by descending order of frequency.

```{r}
freq_words <- alice %>%
  count(lemma) %>%
  arrange(desc(n))

freq_words %>%
  head()
```


## 36. Top-ten frequent words
>Visualize the top-ten frequent words and their frequencies with a chart (e.g., bar chart).

```{r}
freq_words %>%
  head(10) %>%
  ggplot(aes(x = reorder(lemma, -n), y = n)) +
  geom_bar(stat = "identity") +
  labs(x = "", y = "count") +
  theme_minimal()
```



## 37. Top-ten words co-occurring with ‘Alice’
>Extract the list of words that co-occur with the word “Alice”. Visualize with a chart (e.g., bar chart) the top-ten words co-occurring with the word “Alice” and their frequencies.

```{r}
id_alice <- alice %>%
  filter(lemma == "Alice") %>%
  pull(id_sentence) %>%
  unique()

alice %>%
  filter(id_sentence %in% id_alice, lemma != "Alice") %>%
  count(lemma) %>%
  arrange(desc(n)) %>%
  head(10) %>%
  ggplot(aes(x = reorder(lemma, -n), y = n)) +
  geom_bar(stat = "identity") +
  labs(x = "", y = "count") +
  theme_minimal()
```


## 38. Histogram
>Draw a histogram of word frequency (x-axis is a scalar range representing a frequency ranging from 1 to the largest frequency of a given word in the entire corpus, and the y-axis is the count of unique words that fall into the count of the x value).

```{r}
alice %>%
  count(lemma) %>%
  ggplot(aes(n)) +
  geom_histogram() +
  labs(x = "frequency (count)", y = "number of words") +
  theme_minimal()
```


## 39. Zipf’s law
>Plot a log-log graph with the x-axis being rank order and the y-axis being frequency.

```{r}
alice %>%
  count(lemma) %>%
  mutate(rank = dense_rank(desc(n))) %>%
  ggplot(aes(x = log(rank), y = log(n))) +
  geom_point() +
  theme_minimal()
  
```















